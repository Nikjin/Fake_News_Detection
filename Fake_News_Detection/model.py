# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IIJd6MZ6wXi2v2o3jArf0OyOZaz_Hzjc
"""

import numpy as np
import pandas as pd

import keras
from keras import Sequential
from keras.models import load_model
from keras.layers import LSTM, Bidirectional, Dense, Embedding, Dropout
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

import pickle

df = pd.read_csv("clean.csv")

df.head(10)

df= df.dropna()
#df.isnull().sum()

"""##PREPARE THE DATA BY PERFORMING TOKENIZATION AND PADDING"""

# split data into test and train 
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(df.clean, df.label, test_size = 0.2, random_state =42)

y_train = np.asarray(y_train)
y_test = np.asarray(y_test)

vocab_size = 20000
embedding_dim = 120

tokenizer = Tokenizer(num_words = vocab_size)
tokenizer.fit_on_texts(x_train)
train_sequences = tokenizer.texts_to_sequences(x_train) #applying the tokenizer
test_sequences = tokenizer.texts_to_sequences(x_test)

with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

print("The encoding for document\n",df.clean[0],"\n is : ",train_sequences[0])

padded_train = pad_sequences(train_sequences,maxlen = 40, padding = 'post', truncating = 'post')

padded_test = pad_sequences(test_sequences,maxlen = 40, truncating = 'post', padding = 'post')

"""## Building Model"""

#using bi directional lstm
model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(Bidirectional(LSTM(128)))
model.add(Dropout(0.3))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(1, activation='sigmoid'))
model.summary()

'''# using normal lstm
model = Sequential()
model.add(Embedding(vocab_size,embedding_dim))
model.add(Dropout(0.3))
model.add(LSTM(100))
model.add(Dropout(0.3))
model.add(Dense(64,activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model.summary())
'''

model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

model.fit(padded_train, y_train, batch_size = 64, validation_data= (padded_test, y_test), epochs = 2) #bi - 2 epochs


model.save('model.h5')  # creates a HDF5 file 'my_model.h5'
#del model  # deletes the existing model

# returns a compiled model
# identical to the previous one
#model = load_model('model.h5')
